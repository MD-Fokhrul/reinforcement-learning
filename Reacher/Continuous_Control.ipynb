{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reacher\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will train DDPG agent to navigate robotic manipulator to goal destination.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages and srarting Unity environment.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils.config \n",
    "import pprint\n",
    "import torch\n",
    "\n",
    "\n",
    "env = UnityEnvironment(file_name='Reacher_Linux/Reacher.x86')\n",
    "\n",
    "# Get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create instance of agent\n",
    "\n",
    "Agent's hyperparameters are saved and loaded from config.py file in utils folder. Current values are result of selected after hyperparameter tuing. But you can try different hyperparameter values if you want.  \n",
    "\n",
    "If you just want to see the trained agent jump to cell 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.ddpg_agent import DDPG\n",
    "\n",
    "# Load parameters from file\n",
    "hparams = utils.config.HYPERPARAMS['DDPG']\n",
    "params = utils.config.TRAINPARAMS['Reacher']\n",
    "\n",
    "# Create agent instance\n",
    "agent = DDPG(hparams)\n",
    "print(\"Created agent with following hyperparameter values:\")\n",
    "pprint.pprint(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train an egent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset and set environment to training mode\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# Maximum number of training episodes\n",
    "n_episodes = params['n_episodes']\n",
    "# Initialize epsilon\n",
    "#epsilon = params['epsilon_start']\n",
    "# List containing scores from each episode\n",
    "scores = []\n",
    "# Store last 100 scores\n",
    "scores_window = deque(maxlen=params['scores_window_size'])\n",
    "\n",
    "#states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "#scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "\n",
    "# Filename string\n",
    "filename = \"{:s}_lra{:.0E}_lrc{:.0E}_batch{:d}_fc:{:d}:{:d}_solved{:d}\"\n",
    "\n",
    "# Train loop\n",
    "for i_episode in range(1, n_episodes+1):\n",
    "    # Reset environment\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "    # Observe current state\n",
    "    state = env_info.vector_observations[0]\n",
    "\n",
    "    # Reset score and done flag\n",
    "    score = 0\n",
    "    done = False\n",
    "\n",
    "    # Loop each episode\n",
    "    while not done:\n",
    "\n",
    "        # Select action with e-greedy policy\n",
    "        action = agent.act(state)\n",
    "\n",
    "        # Take action\n",
    "        env_info = env.step(action)[brain_name]\n",
    "\n",
    "        # Observe the next state\n",
    "        next_state = env_info.vector_observations[0]\n",
    "\n",
    "        # Get the reward\n",
    "        reward = env_info.rewards[0]\n",
    "\n",
    "        # Check if episode is finished\n",
    "        done = env_info.local_done[0]\n",
    "\n",
    "        # Store experience\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "\n",
    "        # State transition\n",
    "        state = next_state\n",
    "\n",
    "        # Update total score\n",
    "        score += reward\n",
    "\n",
    "    # Save most recent score\n",
    "    scores_window.append(score)\n",
    "    scores.append([score, np.mean(scores_window)])\n",
    "\n",
    "    # Decay epsilon\n",
    "    #epsilon = max(params['epsilon_final'], params['epsilon_decay']*epsilon)\n",
    "\n",
    "    # Print learning progress\n",
    "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "    if i_episode % params['scores_window_size'] == 0:\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "    if np.mean(scores_window)>=params['solve_score']:\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "        filename = filename.format(hparams['name'], hparams['lr_actor'], hparams['lr_critic'], hparams['batch_size'],\n",
    "                                   hparams['fc1_units'], hparams['fc2_units'], i_episode-100)\n",
    "        torch.save(agent.actor_local.state_dict(), 'models/{:s}_actor.pth'.format(filename))\n",
    "        torch.save(agent.critic_local.state_dict(), 'models/{:s}_critic.pth'.format(filename))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plot and save the score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save score\n",
    "df = pd.DataFrame(scores,columns=['scores','average_scores'])\n",
    "df.to_csv('scores/{:s}.csv'.format(filename))\n",
    "\n",
    "# Plot scores\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.axhline(30, color='red', lw=1, alpha=0.3)\n",
    "plt.plot( df.index, 'scores', data=df, color='lime', lw=1, label=\"score\", alpha=0.4)\n",
    "plt.plot( df.index, 'average_scores', data=df, color='green', lw=2, label=\"average score\")\n",
    "# Set labels and legends\n",
    "plt.xlabel('Episode')\n",
    "plt.xlim(0, len(df.index))\n",
    "plt.xticks(50*np.arange(int(len(df.index)/50+1)))\n",
    "plt.ylabel('Score')\n",
    "#plt.yticks(3*np.arange(8))\n",
    "plt.title(filename)\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "plt.legend()\n",
    "# Save figure\n",
    "plt.savefig('docs/plots/{:s}.png'.format(filename), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, close the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Watch smart agent\n",
    "If you are skipped training, please specify filename for pre-trained network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speed (False: Real time, True: Fast)\n",
    "train_mode = False\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=train_mode)[brain_name]\n",
    "\n",
    "# Load learned model weight. Load 'DDPG_lra1E-04_lrc1E-04_batch128_fc:256:128_solved256' for optimal model\n",
    "#filename = 'DDPG_lra1E-04_lrc1E-04_batch128_fc:256:128_solved256'\n",
    "agent.actor_local.load_state_dict(torch.load('models/{}_actor.pth'.format(filename)))\n",
    "agent.critic_local.load_state_dict(torch.load('models/{}_critic.pth'.format(filename)))\n",
    "\n",
    "# Number of episodes to run\n",
    "n_episodes = 1\n",
    "\n",
    "# Run loop\n",
    "for i_episode in range(1, n_episodes+1):\n",
    "    # Reset environment\n",
    "    env_info = env.reset(train_mode=train_mode)[brain_name]\n",
    "\n",
    "    # Observe current state\n",
    "    state = env_info.vector_observations[0]\n",
    "\n",
    "    # Reset score and done flag\n",
    "    score = 0\n",
    "    done = False\n",
    "\n",
    "    # Episode loop\n",
    "    while not done:\n",
    "\n",
    "        # Select action with greedy policy\n",
    "        action = agent.act(state)\n",
    "\n",
    "        # Take action\n",
    "        env_info = env.step(action)[brain_name]\n",
    "\n",
    "        # Observe the next state\n",
    "        next_state = env_info.vector_observations[0]\n",
    "\n",
    "        # Get the reward\n",
    "        reward = env_info.rewards[0]\n",
    "\n",
    "        # Check if episode is finished\n",
    "        done = env_info.local_done[0]\n",
    "\n",
    "        # State transition\n",
    "        state = next_state\n",
    "\n",
    "        # Update total score\n",
    "        score += reward\n",
    "\n",
    "    # Print episode summary\n",
    "    print('Episode %d Score:%d'%(i_episode, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
