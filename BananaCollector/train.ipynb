{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banana collector\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will train DQN agent to pick up yellow bananas while avoiding blue ones in Unity ML-Agents environment.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "#from agents import *\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import config \n",
    "import pprint\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana_Linux/Banana.x86_64\")\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configure  and create instance of DQN agent\n",
    "\n",
    "DQN agent's hyperparameters are saved and loaded from config.py file. Current values are result of coarse hyperparameter tuing. Feel free to try different hyperparameter values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.ddqn_agent import DDQN\n",
    "\n",
    "# Load parameters from file\n",
    "hparams = config.HYPERPARAMS['DDQN']\n",
    "params = config.TRAINPARAMS['BananaCollector']\n",
    "\n",
    "# Create agent instance\n",
    "agent = DDQN(hparams)\n",
    "print(\"Created agent with following hyperparameter values:\")\n",
    "pprint.pprint(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train DQN agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum number of training episodes\n",
    "n_episodes = params['n_episodes']\n",
    "# Initialize epsilon\n",
    "epsilon = params['epsilon_start']\n",
    "# List containing scores from each episode\n",
    "scores = []\n",
    "# Store last 100 scores\n",
    "scores_window = deque(maxlen=params['scores_window_size'])\n",
    "\n",
    "# Train loop\n",
    "for i_episode in range(1, n_episodes+1):\n",
    "    # Reset environment\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "    # Observe current state\n",
    "    state = env_info.vector_observations[0]\n",
    "\n",
    "    # Reset score and done flag\n",
    "    score = 0\n",
    "    done = False\n",
    "\n",
    "    # Loop each episode\n",
    "    while not done:\n",
    "\n",
    "        # Select action with e-greedy policy\n",
    "        action = agent.act(state, epsilon)\n",
    "\n",
    "        # Take action\n",
    "        env_info = env.step(action)[brain_name]\n",
    "\n",
    "        # Observe the next state\n",
    "        next_state = env_info.vector_observations[0]\n",
    "\n",
    "        # Get the reward\n",
    "        reward = env_info.rewards[0]\n",
    "\n",
    "        # Check if episode is finished\n",
    "        done = env_info.local_done[0]\n",
    "\n",
    "        # Store experience\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "\n",
    "        # State transition\n",
    "        state = next_state\n",
    "\n",
    "        # Update total score\n",
    "        score += reward\n",
    "\n",
    "    # Save most recent score\n",
    "    scores_window.append(score)\n",
    "    scores.append([score, np.mean(scores_window)])\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(params['epsilon_final'], params['epsilon_decay']*epsilon)\n",
    "\n",
    "    # Print learning progress\n",
    "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "    if i_episode % params['scores_window_size'] == 0:\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "    if np.mean(scores_window)>=params['solve_score']:\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "        # Filename string\n",
    "        filename = \"{:s}_lr{:.1E}_batch{:d}_model{:d}x{:d}_solved{:d}\"\n",
    "        filename = filename.format(hparams['name'], hparams['learning_rate'], hparams['batch_size'], hparams['fc1_units'], hparams['fc2_units'], i_episode-100)\n",
    "        torch.save(agent.qnetwork_local.state_dict(), 'models/{:s}.pth'.format(filename))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plot score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save score\n",
    "df = pd.DataFrame(scores,columns=['scores','average_scores'])\n",
    "df.to_csv('scores/{:s}.csv'.format(filename))\n",
    "\n",
    "# Plot scores\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.axhline(13, color='red', lw=1, alpha=0.3)\n",
    "plt.plot( df.index, 'scores', data=df, color='lime', lw=1, label=\"score\", alpha=0.4)\n",
    "plt.plot( df.index, 'average_scores', data=df, color='green', lw=2, label=\"average score\")\n",
    "# Set labels and legends\n",
    "plt.xlabel('Episode')\n",
    "plt.xlim(0, len(df.index))\n",
    "plt.xticks(50*np.arange(int(len(df.index)/50+1)))\n",
    "plt.ylabel('Score')\n",
    "plt.yticks(3*np.arange(8))\n",
    "plt.title('DQN agent')\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "plt.legend()\n",
    "# Save figure\n",
    "plt.savefig('graphs/{:s}.png'.format(filename), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
